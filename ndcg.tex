\section{NDCG}
\label{sec:ndcg}
For relevance scores, the discounted cumulative gain at $K$ is 
\begin{align*}
  \text{DCG}@K(\sigma, (y_j)_{j=1}^n)
  = \sum_{r=1}^K \frac{G(y_{\sigma^{-1}(r)})}{D(r)}
  = \sum_{i: \sigma(i) \leq K} \frac{G(y_i)}{D(\sigma(i))}.
\end{align*}
(The first sum runs over positions and the second sum runs over
object indices.) The gain function $G$ boosts relevance scores, and is
normally taken to be $G(y) = 2^y - 1$. The discount function $D$ discounts
the importance of the term based on the position of the item, and is
normally taken to be $D(\sigma(i)) = \log(1 + \sigma(i))$. As before, DCG
is the non-truncated version of this, and NDCG normalizes DCG to obtain
a number in $[0,1]$, defined as
\begin{align*}
  \text{NDCG}(\sigma, (y_j)_{j=1}^n)
  = \frac{1}{N((y_j)_{j=1}^n)} DCG(\sigma, (y_j)_{j=1}^n),
\end{align*}
where
\begin{align*}
  N(y) = N((y_j)_{j=1}^n)
  = \frac{1}{\max_{\sigma'} \text{DCG}(\sigma', (y_j)_{j=1}^n)} \text{DCG}(\sigma, (y_j)_{j=1}^n)
\end{align*}
is the normalization factor. 
In many applications, it is more important for the output to rank well for
items that are near the top compared to ones below, and to rank well for
highly relevant items compared to those with low scores; NDCG factors
these in using the discount and gain functions respectively.

If the algorithm is to output a scoring function and has access to relevance
scores as labels, perhaps the simplest idea is to minimize similarity between
the function output and labels; when they are equal, the NCDG is also one.
In~\cite{cossock2008subset}, it was
established that regression works, in the sense that minimizing the least-squares
surrogate
\begin{align*}
  \phi(\vec{s}, y) = \sum_{j=1}^n {(s_j - y_j)}^2
\end{align*}
actually minimizes $1 - \text{NDCG}(\vec{s}, y)$, because we can upper bound
the latter by some multiple of the former. But this upper bound is coarse.
Moreover, such a surrogate is pointwise, in the sense
that training is done on individual objects $(x_j, y_j)$, and does not
exploit potential structure in the problem.
%In~\cite{ndcg-consistency}, it was shown
%that the normalization factor in NDCG has to be incorporated into the
%surrogate loss in order to be consistent -- the main result is that the
%minimizer of a surrogate loss has to 

In what is termed the pairwise approach, the surrogate loss
$\phi(\vec{s}, y)$ decomposes over pairs of objects rather than single objects
as above:
\begin{align*}
  \phi(\vec{s}, y)
  &\stackrel{\Delta}{=}
  \sum_{j,k : i \neq j} \phi'(s_j - s_k, y_j - y_k))
\end{align*}
for some $\phi'$. (This can work for preference graphs by replacing $y_j - y_k$
with the weight $w_{j,k}$.)
The pairwise approach attempts to reduce the problem of
ranking to the problem of classifying a pair as $\pm 1$ (i.e.,
when an object should be preferred over another). Some choices of $\phi'$
are the hinge loss of Ranking SVM~\cite{Herbrich99rankingsvm}
($\phi'(s, y) = \1_{y < 0}(1 - s)_+$),
exponential loss of RankBoost~\cite{Freund98rankboost},
and the logistic loss of RankNet~\cite{Burges05ranknet}.
It is instructive to review how the logistic loss of RankNet is derived.
First, formulate a probability model for the probability
of a pair $(x_j, x_k)$ receiving the label $+1$, based on the function's
output. Then think of $\text{sign}(y_j - y_k)$
for this pair as coming from the underlying distribution that we are learning.
We want the distribution induced by the function's output to be similar to the
underlying distribution, so it makes sense to take $\phi'$ as the KL-divergence
\begin{align*}
  \phi'(s_j - s_k, y_j - y_k)
  = \text{KL}(\Pr[\text{$(x_j, x_k)$ has label $1$} ; s_j - s_k], \Pr[\text{$(x_j, x_k)$ has label $1$} ; \text{sign}(y_j - y_k)]).
\end{align*}
RankNet uses the probability models
\begin{align*}
  \Pr[\text{$(x_j, x_k)$ has label $+1$}; s_j - s_k] &= \frac{1}{1 + e^{-(s_j - s_k)}} \\
  \Pr[\text{$(x_j, x_k)$ has label $+1$} ; \text{sign}(y_j - y_k)] &=
  \begin{cases}
    1 & \text{sign}(y_j - y_k) = 1 \\
    \frac{1}{2} & \text{sign}(y_j - y_k) = 0 \\
    0 & \text{sign}(y_j - y_k) = -1
  \end{cases}
\end{align*}
(The latter can be changed to a proportion over the training set.)
Computing the KL divergence gives $\phi'(s, y) = -\overline{p} s + \log(1 + e^s)$,
where
$\overline{p} = \Pr[\text{$(x_j, x_k)$ has label $+1$} ; \text{sign}(y_j - y_k)]$. 

An important observation of the pairwise approach is that these surrogates
penalize misordered pairs $x_j$, $x_k$ the same regardless of where they
reside in the true sorted list according to $y$. 
Considering that NDCG places more weight on doing well near the top of the list
compared to below, Burges et.\ al.\ adapted the gradient in the gradient descent
used by RankNet to place more weight on improving performance for items near
the top. This method is called LambdaRank, and has inspired a number of
ranking algorithms that are now state-of-the-art~\cite{cross-benchmark}.

How these pairwise approaches relate to a ranking measure like NDCG in general
is less clear than the pointwise case.
In~\cite{chen2009rankingmeasures}, Chen et.\ al.\ resolved this by defining
a notion of an essential loss $\ell(f; \vec{x}, y)$, which is computed as a
weighted sum of $n$ 0-1 misclassification errors. Each of these errors arise
from the classification task of having $f$ output the right object at position
$j$ according to $y$, after the objects at positions $1, \ldots, j-1$ have been
removed. They showed that this essential loss is a proxy bounded by
the surrogate loss and the ranking loss $1 - \text{NDCG}(\vec{s}, \vec{y})$.
Using this idea, they showed that Ranking SVM, RankBoost, and RankNet all
minimize $1 - \text{NDCG}(\vec{s}, y)$.

MOVE TO PD SECTION:
In terms of consistency, Duchi et.\ al.~\cite{duchi2010ranking}
studied the consistency of pairwise
surrogates, and proved that a surrogate is consistent w.r.t. a
loss $\ell(\sigma, G)$ that is sensitive only to changes in rank
(e.g.\ PD loss and NDCG) iff it is $\ell$-calibrated (called edge-consistent
in that paper). Using calibration, they showed that Ranking SVM and RankBoost
are inconsistent w.r.t.\ PD loss, even under a low-noise assumption.
However, empirical success of these pairwise surrogates led to another study
that generalized edge-consistency/calibration to a condition they called
rank-consistency.
Roughly, a surrogate is rank-consistent if all its minimizers
disagree with every non-minimizer of $\ell$ on some pair. Under a condition
on distributions that they call rank-differentiability,
similar to the low-noise condition, weighted pairwise surrogates are consistent
w.r.t.\ PD loss. The surrogates of Ranking SVM, RankBoost, and RankNet, are
all instances, and are hence consistent w.r.t.\ PD loss.
END MOVE.

Without heuristics like the virtual gradient of RankNet, the pairwise approach
does not incorporate bias towards the top. Hence
in~\cite{from-pairwise-to-listwise}, Cao et.\ al.\ proposed the use of 
surrogates that is defined over the whole list of items and hence takes the
whole structure into account -- this is termed the listwise approach.
In that work, like in RankNet, we use probability models, but this time of
\begin{align*}
  &\Pr[\text{$(x_1, \ldots, x_n)$ is ordered according to $\sigma$} ; \vec{s}] \\
  &\Pr[\text{$(x_1, \ldots, x_n)$ is ordered according to $\sigma$} ; y].
\end{align*}
This is thus a generalization of RankNet from pairwise to listwise. To make
computation tractable, the two models are replaced with just a model of
$x_j$ being ranked first, but the loss is still the KL divergence of these two.
This method is called ListNet, and also ranks among the state-of-the-art in the
cross-benchmark study. Similar ideas come up in other work, e.g.\ ListMLE
uses just a model for 

In~\cite{ndcg-consistency}, it was established that the normalization factor
in NDCG is crucial for consistency; the main result is that a surrogate is
consistent iff for all $\X$-conditional distributions on relevance scores,
the minimizer achieving the conditional surrogate risk
respects the order induced by the expected value of $\frac{G(y)}{N(y)}$.
It was found that the
least-squares loss and ListNet mentioned above are both not consistent, because
they do not use the right normalization; corrections to make them consistent
were then proposed.

Recent work this year addressed the implicit loss in LambdaRank~\cite{cikm}.
Generalizing the probability models above, they define a probability model for
$\Pr[y \mid \vec{s}]$ as a mixture model, i.e.\
\begin{align*}
  \Pr[y \mid \vec{s}] = \sum_{\sigma \in P_n} \Pr(y ; \vec{s}, \sigma) \Pr(\sigma ; \vec{s}).
\end{align*}
LambdaRank can be seen as an instance of this model, giving it a theoretical
explanation. 
However, it is still unclear whether this surrogate is consistent; this is an
avenue for future research. 
