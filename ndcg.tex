\section{NDCG}
\label{sec:ndcg}
For relevance scores, the discounted cumulative gain at $K$ is 
\begin{align*}
  \text{DCG}@K(\sigma, (y_j)_{j=1}^n)
  = \sum_{r=1}^K \frac{G(y_{\sigma^{-1}(r)})}{D(r)}
  = \sum_{i: \sigma(i) \leq K} \frac{G(y_i)}{D(\sigma(i))}.
\end{align*}
(The first sum runs over positions and the second sum runs over
object indices.) The gain function $G$ boosts relevance scores, and is
normally taken to be $G(y) = 2^y - 1$. The discount function $D$ discounts
the importance of the term based on the position of the item, and is
normally taken to be $D(\sigma(i)) = \log(1 + \sigma(i))$. As before, DCG
is the non-truncated version of this, and NDCG normalizes DCG to obtain
a number in $[0,1]$, defined as
\begin{align*}
  \text{NDCG}(\sigma, (y_j)_{j=1}^n)
  = \frac{1}{N((y_j)_{j=1}^n)} DCG(\sigma, (y_j)_{j=1}^n),
\end{align*}
where
\begin{align*}
  N(y) = N((y_j)_{j=1}^n)
  = \frac{1}{\max_{\sigma'} \text{DCG}(\sigma', (y_j)_{j=1}^n)} \text{DCG}(\sigma, (y_j)_{j=1}^n)
\end{align*}
is the normalization factor. 
In many applications, it is more important for the output to rank well for
items that are near the top compared to ones below, and to rank well for
highly relevant items compared to those with low scores; NDCG factors
these in using the discount and gain functions respectively.

If the algorithm is to output a scoring function and has access to relevance
scores as labels, perhaps the simplest idea is to minimize similarity between
the function output and labels; when they are equal, the NCDG is also one.
In~\cite{cossock2008subset}, Cossock and Zhang
established that regression works, in the sense that minimizing the least-squares
surrogate
\begin{align*}
  \phi(\vec{s}, y) = \sum_{j=1}^n {(s_j - y_j)}^2
\end{align*}
actually minimizes $1 - \text{NDCG}(\vec{s}, y)$, because we can upper bound
the latter by some multiple of the former. But this upper bound is coarse.
Moreover, such a surrogate is pointwise, in the sense
that training is done on individual objects $(x_j, y_j)$, and does not
exploit potential structure in the problem.
%In~\cite{ndcg-consistency}, it was shown
%that the normalization factor in NDCG has to be incorporated into the
%surrogate loss in order to be consistent -- the main result is that the
%minimizer of a surrogate loss has to 

In what is termed the pairwise approach, the surrogate loss
$\phi(\vec{s}, y)$ decomposes over pairs of objects rather than single objects
as above:
\begin{align*}
  \phi(\vec{s}, y)
  &\stackrel{\Delta}{=}
  \sum_{j,k : i \neq j} \phi'(s_j - s_k, y_j - y_k))
\end{align*}
for some $\phi'$. (This can work for preference graphs by replacing $y_j - y_k$
with the weight $w_{j,k}$.)
The pairwise approach attempts to reduce the problem of
ranking to the problem of classifying a pair as $\pm 1$ (i.e.,
when an object should be preferred over another). Some choices of $\phi'$
are the hinge loss of Ranking SVM~\cite{Herbrich99rankingsvm},
%($\phi'(s, y) = \1_{y < 0}(1 - s)_+$),
exponential loss of RankBoost~\cite{Freund98rankboost},
and the logistic loss of RankNet~\cite{Burges05ranknet}.

It is instructive to review how the logistic loss of RankNet is derived, because
of its influence on current state-of-the-art approaches.
First, formulate a probability model for the probability
of a pair $(x_j, x_k)$ receiving the label $+1$, based on the function's
output. Then think of $\text{sign}(y_j - y_k)$
for this pair as coming from the underlying distribution that we are learning.
We want the distribution induced by the function's output to be similar to the
underlying distribution, so it makes sense to take $\phi'$ as the KL-divergence
\begin{align*}
  \phi'(s_j - s_k, y_j - y_k)
  = \text{KL}(\Pr[\text{$(x_j, x_k)$ has label $1$} ; s_j - s_k], \Pr[\text{$(x_j, x_k)$ has label $1$} ; \text{sign}(y_j - y_k)]).
\end{align*}
RankNet uses the probability models
\begin{align*}
  \Pr[\text{$(x_j, x_k)$ has label $+1$}; s_j - s_k] &= \frac{1}{1 + e^{-(s_j - s_k)}} \\
  \Pr[\text{$(x_j, x_k)$ has label $+1$} ; \text{sign}(y_j - y_k)] &=
  \begin{cases}
    1 & \text{sign}(y_j - y_k) = 1 \\
    \frac{1}{2} & \text{sign}(y_j - y_k) = 0 \\
    0 & \text{sign}(y_j - y_k) = -1
  \end{cases}
\end{align*}
(The latter can be changed to a proportion over the training set.)
Computing the KL divergence gives $\phi'(s, y) = -\overline{p} s + \log(1 + e^s)$,
where
$\overline{p} = \Pr[\text{$(x_j, x_k)$ has label $+1$} ; \text{sign}(y_j - y_k)]$.
RankNet then uses stochastic gradient descent to minimize this loss over the
training set.

Analyzing how these pairwise approaches relate to a ranking measure like
NDCG in general is more difficult than the pointwise case.
In 2009, Chen et.\ al.~\cite{chen2009rankingmeasures} shed some light on this
by defining a notion of an essential loss $\ell(f; \vec{x}, y)$, which
is computed as a weighted sum of $n$ 0-1 misclassification errors.
Each of these errors arise from using $f$ to predict the right object at
position $j$ according to $y$,
after the objects at positions $1, \ldots, j-1$ (according to $y$) have been
removed. They showed that this essential loss is a proxy bounded by
the surrogate loss and the ranking loss $1 - \text{NDCG}(\vec{s}, \vec{y})$.
Using this idea, they showed that Ranking SVM, RankBoost, and RankNet all
minimize $1 - \text{NDCG}(\vec{s}, y)$.

An observation of the pairwise approach, however, is that pairwise surrogates
penalize misordered pairs $x_j$, $x_k$ the same regardless of where they
reside in the true sorted list according to $y$.
Considering that NDCG places more weight on doing well near the top of the list
compared to below, Burges et.\ al. adapted the gradient in the gradient descent
used by RankNet to place more weight on improving performance for items near
the top. This method is called LambdaRank~\cite{Burges2007lambdarank},
and has inspired a number of
ranking algorithms like LambdaMart~\cite{Burges2010ranknetlambdamart} and
LambdaNeuralRank~\cite{papini2012neuralrank}
that are now state-of-the-art~\cite{tax2015benchmark}. However, they
have been difficult to analyze because the virtual gradient corresponds to
an implicit loss function.

In~\cite{cao2007-pairwiselistwise}, Cao et.\ al.\ proposed using 
surrogates that are defined over the whole list of items and deals with ranking
more directly. This is termed the listwise approach. 
(POSSIBLY INCLUDE BACKPOINTER TO XIA HERE).
In that work, like in RankNet, we use probability models, but this time of
\begin{align*}
  &\Pr[\text{($x_1, \ldots, x_n$) is ordered according to $\sigma$} ; \vec{s}] \\
  &\Pr[\text{($x_1, \ldots, x_n$) is ordered according to $\sigma$} ; y].
\end{align*}
This is thus a generalization of RankNet from pairwise to listwise. To make
computation tractable, the two models are replaced with just a model of
$x_j$ being ranked first, but the loss is still the KL divergence of these two.
This method is called ListNet, and also ranks among the
state-of-the-art~\cite{tax2015benchmark}.

Similar ideas of modeling distributions over permutations and scores 
have come up in other work. For example, ListMLE~\cite{xia2008listwise}
compares function output $\vec{s}$ with relevance scores $y$ directly
by defining a probability model for $\Pr[y \mid \vec{s}]$, and using the
negative log-likelihood of the sample as the loss.
SoftRank~\cite{taylor2008softrank} likewise models a distribution over scores
$\Pr[y \mid \vec{s}]$, but uses a Gaussian model (mean-centered at $\vec{s}$),
and uses this to induce a distribution $\Pr[\sigma ; \vec{s}]$ over rankings;
then the loss is given by a ``soft'' NDCG:
\begin{align*}
  \text{SoftNDCG}(\vec{s}, y) = \sum_{\sigma} \text{NDCG}(\sigma, y) \Pr[\sigma ; \vec{s}].
\end{align*}
The key point is that this Gaussian smoothing makes the function differentiable,
allowing for direct optimization via gradient descent.

The use of these probability models have recently culminated in a deeper understanding
of LambdaRank, unifying the heuristic modification inspired from the
pairwise view with the listwise view.
Building on SoftRank, recent work this year addressed the implicit loss in
LambdaRank~\cite{wang2018lambdaloss}.
Generalizing these probability models, they define a probability model
for $\Pr[y \mid \vec{s}]$ as the following mixture model:
\begin{align*}
  \Pr[y \mid \vec{s}] = \sum_{\sigma \in P_n} \Pr(y ; \vec{s}, \sigma) \Pr(\sigma ; \vec{s})
\end{align*}
LambdaRank can be seen as an instance of this LambdaLoss model,
giving it an explicit form and a theoretical explanation. 

%The essential-loss proxy of Chen et. al.~\cite{chen2009rankingmeasures}
%applies to show that the ListMLE loss upper-bounds
%$1 - \text{NDCG}(\vec{s}, y)$.

In terms of consistency results for all these algorithms, Ravikumar
et.\ al.~\cite{ravikumar2011ndcg} showed that the normalization factor
in NDCG is crucial for consistency; their main result is that a surrogate is
consistent iff for all $\X$-conditional distributions on relevance scores,
the minimizer achieving the conditional surrogate risk
respects the order induced by the expected value of $\frac{G(y)}{N(y)}$.
It was found that the pointwise least-squares loss and ListNet mentioned above
are actually not consistent because they do not use the right normalization, but
corrections can be made to make them consistent.
Consistency of the surrogate used in LambdaRank (as explained by LambdaLoss)
is unclear, and is an avenue for future research.
